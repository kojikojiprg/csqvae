# dataset
name: "mnist"
n_labeled_samples: 6000
x_dim: 784  # 28 * 28
x_shape: [1, 28, 28]
patch_size: [7, 7]

# clustering
n_clusters: 10

# quantization
book_size: 128
sigma_q_init: 10
temp_init: 1.0
temp_decay: 0.00001
temp_min: 0.001

# encoder-decoder
n_resblocks: 2
latent_dim: 64
latent_size: [7, 7]

# classification head
n_blocks_cls: 3
latent_dim_cls: 256
n_heads_cls: 16
dropout: 0.1

# diffusion
noise_steps: 1000
beta_start: 0.001
beta_end: 0.02
latent_dim_diffusion: 256
n_blocks_dit: 6
n_heads_dit: 16

loss:
  csqvae:
    lmd_x: 1
    lmd_klc: 1
    lmd_kld: 1
    lmd_z: 1
    lmd_ldt: 1
    lmd_c_elbo: 10
    lmd_c_real: 10
  finetuning:
    lmd_z: 1
    lmd_ldt: 1

optim:
  sqvae:
    epochs: 100
    batch_size: 32
    accumulate_grad_batches: 1
    lr: 0.001
  diffusion:
    epochs: 100
    batch_size: 32
    accumulate_grad_batches: 1
    lr: 0.0001
  csqvae:
    epochs: 100
    batch_size: 128
    accumulate_grad_batches: 1
    lr: 0.0004
    lr_min: 0.00001
    warmup_t: 10
    warmup_lr_init: 0.000001
  finetuning:
    epochs: 100
    batch_size: 128
    accumulate_grad_batches: 1
    lr: 0.00001

  num_workers: 4
